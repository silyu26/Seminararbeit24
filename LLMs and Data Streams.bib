@Article{Liu_2023,
  author    = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhao, Lin and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
  journal   = {Meta-Radiology},
  title     = {Summary of ChatGPT-Related research and perspective towards the future of large language models},
  year      = {2023},
  issn      = {2950-1628},
  month     = sep,
  number    = {2},
  pages     = {100017},
  volume    = {1},
  abstract  = {This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
  doi       = {https://doi.org/10.1016/j.metrad.2023.100017},
  file      = {:C\:/Users/11584/Downloads/Summary ChatGPT.pdf:PDF},
  publisher = {Elsevier BV},
}

@Misc{https://doi.org/10.48550/arxiv.2307.08225,
  author    = {Zhang, Shuhao and Zeng, Xianzhi and Wu, Yuhao and Yang, Zhonghao},
  title     = {Harnessing Scalable Transactional Stream Processing for Managing Large Language Models [Vision]},
  year      = {2023},
  abstract  = {Large Language Models (LLMs) have demonstrated extraordinary performance across a broad array of applications, from traditional language processing tasks to interpreting structured sequences like time-series data. Yet, their effectiveness in fast-paced, online decision-making environments requiring swift, accurate, and concurrent responses poses a significant challenge. This paper introduces TStreamLLM, a revolutionary framework integrating Transactional Stream Processing (TSP) with LLM management to achieve remarkable scalability and low latency. By harnessing the scalability, consistency, and fault tolerance inherent in TSP, TStreamLLM aims to manage continuous & concurrent LLM updates and usages efficiently. We showcase its potential through practical use cases like real-time patient monitoring and intelligent traffic management. The exploration of synergies between TSP and LLM management can stimulate groundbreaking developments in AI and database research. This paper provides a comprehensive overview of challenges and opportunities in this emerging field, setting forth a roadmap for future exploration and development.},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2307.08225},
  file      = {:C\:/Users/11584/Downloads/TStreamLLM.pdf:PDF},
  keywords  = {Databases (cs.DB), Artificial Intelligence (cs.AI), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Zhang_2024,
  author    = {Zhang, Kunpeng and Zhou, Feng and Wu, Lan and Xie, Na and He, Zhengbing},
  journal   = {Information Fusion},
  title     = {Semantic understanding and prompt engineering for large-scale traffic data imputation},
  year      = {2024},
  issn      = {1566-2535},
  month     = feb,
  pages     = {102038},
  volume    = {102},
  abstract  = {Intelligent Transportation Systems (ITS) face the formidable challenge of large-scale missing data, particularly in the imputation of traffic data. Existing studies have mainly relied on modeling network-level spatiotemporal correlations to address this issue. However, these methods often overlook the rich semantic information (e.g., road infrastructure, sensor location, etc.) inherent in road networks when capturing network-wide spatiotemporal correlations. We address this limitation by presenting the Graph Transformer-based Traffic Data Imputation (GT-TDI) model, which imputes missing values in extensive traffic data by leveraging spatiotemporal semantic understanding of road networks. The proposed model leverages semantic descriptions that capture the spatial and temporal dynamics of traffic across road networks, enhancing its capacity to infer comprehensive spatiotemporal relationships. Moreover, to augment the modelâ€™s capabilities, we employ a Large Language Model (LLM) and prompt engineering to enable natural and intuitive interactions with the traffic data imputation system, allowing users to query and request in plain language, without requiring expert knowledge or complex mathematical models. The proposed model, GT-TDI, utilizes Graph Neural Networks (GNN) and Transformer architectures to perform large-scale traffic data imputation using deficient observations, sensor social connectivity, and semantic descriptions as inputs. We evaluate the GT-TDI model on the PeMS freeway dataset and benchmark it against cutting-edge models. The experimental evidence demonstrates that GT-TDI surpasses the cutting-edge approaches in scenarios with intricate patterns and varying rates of missing data.},
  doi       = {https://doi.org/10.1016/j.inffus.2023.102038},
  file      = {:C\:/Users/11584/Downloads/Traffic data.pdf:PDF},
  publisher = {Elsevier BV},
}

@Article{https://doi.org/10.48550/arxiv.2307.14385,
  author    = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo},
  title     = {Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data},
  year      = {2023},
  abstract  = {Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2307.14385},
  file      = {:C\:/Users/11584/Downloads/Mental Health.pdf:PDF},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, H.5.2; I.2.m, 68U35},
  publisher = {arXiv},
}

@Misc{https://doi.org/10.48550/arxiv.2403.06139,
  author    = {Zhang, Xin and Zhang, Linhai and Zhou, Deyu and Xu, Guoqiang},
  title     = {Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity},
  year      = {2024},
  abstract  = {Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles. However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality synthetic data to effectively address sparsity across different categories. Experimental results on three real datasets demonstrate significant performance improvements, with synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%, respectively.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2403.06139},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Misc{https://doi.org/10.48550/arxiv.2402.01364,
  author    = {Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  title     = {Continual Learning for Large Language Models: A Survey},
  year      = {2024},
  abstract  = {Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2402.01364},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Misc{https://doi.org/10.48550/arxiv.2005.14165,
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  title     = {Language Models are Few-Shot Learners},
  year      = {2020},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2005.14165},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Gama_2012,
  author    = {Gama, JoÃ£o and SebastiÃ£o, Raquel and Rodrigues, Pedro Pereira},
  journal   = {Machine Learning},
  title     = {On evaluating stream learning algorithms},
  year      = {2012},
  issn      = {1573-0565},
  month     = oct,
  number    = {3},
  pages     = {317--346},
  volume    = {90},
  abstract  = {Most streaming decision models evolve continuously over time, run in resource-aware environments, and detect and react to changes in the environment generating data. One important issue, not yet convincingly addressed, is the design of experimental work to evaluate and compare decision models that evolve over time. This paper proposes a general framework for assessing predictive stream learning algorithms. We defend the use of prequential error with forgetting mechanisms to provide reliable error estimators. We prove that, in stationary data and for consistent learning algorithms, the holdout estimator, the prequential error and the prequential error estimated over a sliding window or using fading factors, all converge to the Bayes error. The use of prequential error with forgetting mechanisms reveals to be advantageous in assessing performance and in comparing stream learning algorithms. It is also worthwhile to use the proposed methods for hypothesis testing and for change detection. In a set of experiments in drift scenarios, we evaluate the ability of a standard change detection algorithm to detect change using three prequential error estimators. These experiments point out that the use of forgetting mechanisms (sliding windows or fading factors) are required for fast and efficient change detection. In comparison to sliding windows, fading factors are faster and memoryless, both important requirements for streaming applications. Overall, this paper is a contribution to a discussion on best practice for performance assessment when learning is a continuous process, and the decision models are dynamic and evolve over time.},
  doi       = {https://doi.org/10.1007/s10994-012-5320-9},
  publisher = {Springer Science and Business Media LLC},
}

@Article{https://doi.org/10.48550/arxiv.2110.03215,
  author    = {Jang, Joel and Ye, Seonghyeon and Yang, Sohee and Shin, Joongbo and Han, Janghoon and Kim, Gyeonghun and Choi, Stanley Jungkyu and Seo, Minjoon},
  title     = {Towards Continual Knowledge Learning of Language Models},
  year      = {2021},
  abstract  = {Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2110.03215},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InBook{https://doi.org/10.4230/dfu.vol5.10452.275,
  author    = {Geisler, Sandra},
  publisher = {Schloss Dagstuhl â€“ Leibniz-Zentrum fÃ¼r Informatik},
  title     = {Data Stream Management Systems},
  year      = {2013},
  abstract  = {In many application fields, such as production lines or stock analysis, it is substantial to create
and process high amounts of data at high rates. Such continuous data flows with unknown
size and end are also called data streams. The processing and analysis of data streams are a
challenge for common data management systems as they have to operate and deliver results in real
time. Data Stream Management Systems (DSMS), as an advancement of database management
systems, have been implemented to deal with these issues. DSMS have to adapt to the notion
of data streams on various levels, such as query languages, processing or optimization. In this
chapter we give an overview of the basics of data streams, architecture principles of DSMS and
the used query languages. Furthermore, we specifically detail data quality aspects in DSMS as
these play an important role for various applications based on data streams. Finally, the chapter
also includes a list of research and commercial DSMS and their key properties.},
  copyright = {Creative Commons Attribution 3.0 Unported license},
  doi       = {http://dx.doi.org/10.4230/DFU.Vol5.10452.275},
  keywords  = {Data Streams, Data Stream Management, Data Quality, Query Languages},
  language  = {en},
}

@Comment{jabref-meta: databaseType:bibtex;}
